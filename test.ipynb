{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0806, -0.0136, -0.1307,  0.3103],\n",
       "          [ 0.1510, -1.3213,  0.1075, -1.0196],\n",
       "          [-0.0742, -0.6296, -0.1471,  0.6657],\n",
       "          ...,\n",
       "          [ 1.3830, -0.1695, -1.2322,  0.7343],\n",
       "          [-0.4266,  0.5297,  0.6739, -0.0350],\n",
       "          [ 0.1257,  0.4643,  0.8724, -0.2311]],\n",
       " \n",
       "         [[-1.0071, -0.6358,  0.3785,  0.0262],\n",
       "          [ 0.3602,  0.1215,  0.2305,  0.1926],\n",
       "          [-0.2888,  0.5258, -0.2505,  0.1561],\n",
       "          ...,\n",
       "          [-1.5330,  0.6014,  0.3106, -0.1281],\n",
       "          [-0.3295, -0.4912,  0.0210,  0.1122],\n",
       "          [ 0.5342,  0.2143, -0.9008,  0.9386]],\n",
       " \n",
       "         [[ 0.3538, -1.0485,  0.0940, -0.8294],\n",
       "          [-0.6607, -1.3138,  0.3759, -0.5537],\n",
       "          [ 0.0127, -0.0154,  0.1092, -0.2461],\n",
       "          ...,\n",
       "          [ 0.1495,  0.5793,  0.5567,  0.0289],\n",
       "          [ 1.0528,  0.4189,  0.7282,  0.1530],\n",
       "          [ 0.1302,  0.5135, -0.7641,  0.2144]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.2416,  0.0590, -0.6417,  0.8473],\n",
       "          [-0.9111, -0.0879,  0.5210,  0.4112],\n",
       "          [ 0.0458,  0.7473,  1.1128, -0.3511],\n",
       "          ...,\n",
       "          [ 0.0298, -0.0076, -0.5972,  0.7275],\n",
       "          [ 0.4679, -0.0098, -0.4401,  0.8520],\n",
       "          [-0.6502,  0.9297,  1.4711, -0.3184]],\n",
       " \n",
       "         [[-0.2048, -0.6269,  0.4533, -0.1930],\n",
       "          [ 1.0829,  0.4548, -2.1253,  1.0111],\n",
       "          [ 0.3649,  0.7418,  1.0765, -0.3245],\n",
       "          ...,\n",
       "          [ 0.3770,  0.1663,  0.0101,  0.5094],\n",
       "          [-0.4302,  0.0414,  0.1254, -0.5040],\n",
       "          [ 1.8323,  0.3587, -2.5842,  1.0576]],\n",
       " \n",
       "         [[ 0.6122,  0.3145, -1.3436,  0.5004],\n",
       "          [-0.7885, -0.4776,  0.1917,  0.1116],\n",
       "          [ 0.0200,  0.1746,  0.0838,  0.2799],\n",
       "          ...,\n",
       "          [-0.8769,  0.0047, -0.0250, -0.3228],\n",
       "          [-0.5782,  0.2837,  0.3345,  0.1283],\n",
       "          [-0.8299,  0.1501,  0.3416, -0.1307]]], grad_fn=<ViewBackward0>),\n",
       " tensor(1.5574, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from architecture import Architecture\n",
    "\n",
    "import torch\n",
    "batch_size = 8\n",
    "landmark_count = 478\n",
    "dimension_count = 3\n",
    "embedding_count = 128\n",
    "class_count = 4\n",
    "\n",
    "x = torch.randn((batch_size, landmark_count, dimension_count))\n",
    "y = torch.randint(0, class_count, (batch_size*landmark_count, ))\n",
    "model = Architecture(landmark_count, dimension_count, embedding_count, class_count)\n",
    "model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5875,  0.6471,  0.0272,  0.1892],\n",
       "        [-0.6233, -0.5943,  0.6315,  0.0103],\n",
       "        [-0.1458, -0.4737,  0.5119, -0.5376],\n",
       "        ...,\n",
       "        [-0.6802,  0.2786,  0.4689,  0.0635],\n",
       "        [ 0.4996, -0.2920, -0.3738, -0.0974],\n",
       "        [ 2.1100, -0.4979, -0.5568, -0.6428]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.rand((batch_size, landmark_count, dimension_count), dtype=torch.float32)\n",
    "model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration      1, Loss: 1.557438\n",
      "Iteration      2, Loss: 1.429172\n",
      "Iteration      3, Loss: 1.413527\n",
      "Iteration      4, Loss: 1.405136\n",
      "Iteration      5, Loss: 1.396014\n",
      "Iteration      6, Loss: 1.389332\n",
      "Iteration      7, Loss: 1.385823\n",
      "Iteration      8, Loss: 1.382180\n",
      "Iteration      9, Loss: 1.378009\n",
      "Iteration     10, Loss: 1.374188\n",
      "Iteration     11, Loss: 1.371689\n",
      "Iteration     12, Loss: 1.370415\n",
      "Iteration     13, Loss: 1.369852\n",
      "Iteration     14, Loss: 1.369208\n",
      "Iteration     15, Loss: 1.368283\n",
      "Iteration     16, Loss: 1.367210\n",
      "Iteration     17, Loss: 1.366024\n",
      "Iteration     18, Loss: 1.364801\n",
      "Iteration     19, Loss: 1.363757\n",
      "Iteration     20, Loss: 1.362938\n",
      "Iteration     21, Loss: 1.362251\n",
      "Iteration     22, Loss: 1.361544\n",
      "Iteration     23, Loss: 1.360825\n",
      "Iteration     24, Loss: 1.360136\n",
      "Iteration     25, Loss: 1.359481\n",
      "Iteration     26, Loss: 1.358873\n",
      "Iteration     27, Loss: 1.358240\n",
      "Iteration     28, Loss: 1.357542\n",
      "Iteration     29, Loss: 1.356834\n",
      "Iteration     30, Loss: 1.356243\n",
      "Iteration     31, Loss: 1.355743\n",
      "Iteration     32, Loss: 1.355235\n",
      "Iteration     33, Loss: 1.354712\n",
      "Iteration     34, Loss: 1.354183\n",
      "Iteration     35, Loss: 1.353695\n",
      "Iteration     36, Loss: 1.353167\n",
      "Iteration     37, Loss: 1.352633\n",
      "Iteration     38, Loss: 1.352063\n",
      "Iteration     39, Loss: 1.351492\n",
      "Iteration     40, Loss: 1.350974\n",
      "Iteration     41, Loss: 1.350475\n",
      "Iteration     42, Loss: 1.349977\n",
      "Iteration     43, Loss: 1.349478\n",
      "Iteration     44, Loss: 1.349002\n",
      "Iteration     45, Loss: 1.348517\n",
      "Iteration     46, Loss: 1.347980\n",
      "Iteration     47, Loss: 1.347455\n",
      "Iteration     48, Loss: 1.346934\n",
      "Iteration     49, Loss: 1.346366\n",
      "Iteration     50, Loss: 1.345799\n",
      "Iteration     51, Loss: 1.345265\n",
      "Iteration     52, Loss: 1.344715\n",
      "Iteration     53, Loss: 1.344139\n",
      "Iteration     54, Loss: 1.343596\n",
      "Iteration     55, Loss: 1.343032\n",
      "Iteration     56, Loss: 1.342441\n",
      "Iteration     57, Loss: 1.341750\n",
      "Iteration     58, Loss: 1.341064\n",
      "Iteration     59, Loss: 1.340391\n",
      "Iteration     60, Loss: 1.339749\n",
      "Iteration     61, Loss: 1.339080\n",
      "Iteration     62, Loss: 1.338415\n",
      "Iteration     63, Loss: 1.337687\n",
      "Iteration     64, Loss: 1.337000\n",
      "Iteration     65, Loss: 1.336313\n",
      "Iteration     66, Loss: 1.335706\n",
      "Iteration     67, Loss: 1.335574\n",
      "Iteration     68, Loss: 1.335328\n",
      "Iteration     69, Loss: 1.333901\n",
      "Iteration     70, Loss: 1.333132\n",
      "Iteration     71, Loss: 1.332795\n",
      "Iteration     72, Loss: 1.332330\n",
      "Iteration     73, Loss: 1.333003\n",
      "Iteration     74, Loss: 1.331552\n",
      "Iteration     75, Loss: 1.330974\n",
      "Iteration     76, Loss: 1.330433\n",
      "Iteration     77, Loss: 1.330508\n",
      "Iteration     78, Loss: 1.332214\n",
      "Iteration     79, Loss: 1.331063\n",
      "Iteration     80, Loss: 1.329538\n",
      "Iteration     81, Loss: 1.328660\n",
      "Iteration     82, Loss: 1.328322\n",
      "Iteration     83, Loss: 1.326682\n",
      "Iteration     84, Loss: 1.326177\n",
      "Iteration     85, Loss: 1.325265\n",
      "Iteration     86, Loss: 1.326104\n",
      "Iteration     87, Loss: 1.326265\n",
      "Iteration     88, Loss: 1.324709\n",
      "Iteration     89, Loss: 1.323806\n",
      "Iteration     90, Loss: 1.322400\n",
      "Iteration     91, Loss: 1.322493\n",
      "Iteration     92, Loss: 1.321142\n",
      "Iteration     93, Loss: 1.320724\n",
      "Iteration     94, Loss: 1.322272\n",
      "Iteration     95, Loss: 1.318918\n",
      "Iteration     96, Loss: 1.319171\n",
      "Iteration     97, Loss: 1.316203\n",
      "Iteration     98, Loss: 1.318408\n",
      "Iteration     99, Loss: 1.320354\n",
      "Iteration    100, Loss: 1.318693\n",
      "Iteration    101, Loss: 1.316055\n",
      "Iteration    102, Loss: 1.319001\n",
      "Iteration    103, Loss: 1.318518\n",
      "Iteration    104, Loss: 1.315659\n",
      "Iteration    105, Loss: 1.316237\n",
      "Iteration    106, Loss: 1.319714\n",
      "Iteration    107, Loss: 1.324894\n",
      "Iteration    108, Loss: 1.318143\n",
      "Iteration    109, Loss: 1.319572\n",
      "Iteration    110, Loss: 1.318853\n",
      "Iteration    111, Loss: 1.320238\n",
      "Iteration    112, Loss: 1.319162\n",
      "Iteration    113, Loss: 1.317295\n",
      "Iteration    114, Loss: 1.318143\n",
      "Iteration    115, Loss: 1.315458\n",
      "Iteration    116, Loss: 1.315562\n",
      "Iteration    117, Loss: 1.313225\n",
      "Iteration    118, Loss: 1.312153\n",
      "Iteration    119, Loss: 1.311447\n",
      "Iteration    120, Loss: 1.309204\n",
      "Iteration    121, Loss: 1.309979\n",
      "Iteration    122, Loss: 1.307850\n",
      "Iteration    123, Loss: 1.307070\n",
      "Iteration    124, Loss: 1.305348\n",
      "Iteration    125, Loss: 1.305540\n",
      "Iteration    126, Loss: 1.308275\n",
      "Iteration    127, Loss: 1.308870\n",
      "Iteration    128, Loss: 1.305214\n",
      "Iteration    129, Loss: 1.303976\n",
      "Iteration    130, Loss: 1.307180\n",
      "Iteration    131, Loss: 1.305548\n",
      "Iteration    132, Loss: 1.305372\n",
      "Iteration    133, Loss: 1.303781\n",
      "Iteration    134, Loss: 1.303043\n",
      "Iteration    135, Loss: 1.301798\n",
      "Iteration    136, Loss: 1.299178\n",
      "Iteration    137, Loss: 1.299887\n",
      "Iteration    138, Loss: 1.298075\n",
      "Iteration    139, Loss: 1.300287\n",
      "Iteration    140, Loss: 1.301079\n",
      "Iteration    141, Loss: 1.296857\n",
      "Iteration    142, Loss: 1.297441\n",
      "Iteration    143, Loss: 1.301380\n",
      "Iteration    144, Loss: 1.299748\n",
      "Iteration    145, Loss: 1.294976\n",
      "Iteration    146, Loss: 1.299747\n",
      "Iteration    147, Loss: 1.297989\n",
      "Iteration    148, Loss: 1.292286\n",
      "Iteration    149, Loss: 1.294857\n",
      "Iteration    150, Loss: 1.293505\n",
      "Iteration    151, Loss: 1.291692\n",
      "Iteration    152, Loss: 1.291245\n",
      "Iteration    153, Loss: 1.289543\n",
      "Iteration    154, Loss: 1.286407\n",
      "Iteration    155, Loss: 1.286914\n",
      "Iteration    156, Loss: 1.290314\n",
      "Iteration    157, Loss: 1.297758\n",
      "Iteration    158, Loss: 1.293482\n",
      "Iteration    159, Loss: 1.291898\n",
      "Iteration    160, Loss: 1.288549\n",
      "Iteration    161, Loss: 1.286356\n",
      "Iteration    162, Loss: 1.285251\n",
      "Iteration    163, Loss: 1.283690\n",
      "Iteration    164, Loss: 1.286017\n",
      "Iteration    165, Loss: 1.283714\n",
      "Iteration    166, Loss: 1.287056\n",
      "Iteration    167, Loss: 1.284662\n",
      "Iteration    168, Loss: 1.282142\n",
      "Iteration    169, Loss: 1.283672\n",
      "Iteration    170, Loss: 1.288475\n",
      "Iteration    171, Loss: 1.284237\n",
      "Iteration    172, Loss: 1.285229\n",
      "Iteration    173, Loss: 1.281829\n",
      "Iteration    174, Loss: 1.279891\n",
      "Iteration    175, Loss: 1.279307\n",
      "Iteration    176, Loss: 1.279996\n",
      "Iteration    177, Loss: 1.278575\n",
      "Iteration    178, Loss: 1.284262\n",
      "Iteration    179, Loss: 1.280213\n",
      "Iteration    180, Loss: 1.277943\n",
      "Iteration    181, Loss: 1.273942\n",
      "Iteration    182, Loss: 1.276639\n",
      "Iteration    183, Loss: 1.277713\n",
      "Iteration    184, Loss: 1.274769\n",
      "Iteration    185, Loss: 1.274988\n",
      "Iteration    186, Loss: 1.278644\n",
      "Iteration    187, Loss: 1.286046\n",
      "Iteration    188, Loss: 1.279793\n",
      "Iteration    189, Loss: 1.278115\n",
      "Iteration    190, Loss: 1.275594\n",
      "Iteration    191, Loss: 1.273972\n",
      "Iteration    192, Loss: 1.272018\n",
      "Iteration    193, Loss: 1.271571\n",
      "Iteration    194, Loss: 1.270064\n",
      "Iteration    195, Loss: 1.272205\n",
      "Iteration    196, Loss: 1.269110\n",
      "Iteration    197, Loss: 1.268651\n",
      "Iteration    198, Loss: 1.265494\n",
      "Iteration    199, Loss: 1.267835\n",
      "Iteration    200, Loss: 1.265779\n",
      "Iteration    201, Loss: 1.271246\n",
      "Iteration    202, Loss: 1.271970\n",
      "Iteration    203, Loss: 1.276554\n",
      "Iteration    204, Loss: 1.281536\n",
      "Iteration    205, Loss: 1.278187\n",
      "Iteration    206, Loss: 1.274019\n",
      "Iteration    207, Loss: 1.273304\n",
      "Iteration    208, Loss: 1.270521\n",
      "Iteration    209, Loss: 1.269004\n",
      "Iteration    210, Loss: 1.269852\n",
      "Iteration    211, Loss: 1.265083\n",
      "Iteration    212, Loss: 1.263150\n",
      "Iteration    213, Loss: 1.260967\n",
      "Iteration    214, Loss: 1.259040\n",
      "Iteration    215, Loss: 1.261464\n",
      "Iteration    216, Loss: 1.259945\n",
      "Iteration    217, Loss: 1.262265\n",
      "Iteration    218, Loss: 1.261764\n",
      "Iteration    219, Loss: 1.261749\n",
      "Iteration    220, Loss: 1.270664\n",
      "Iteration    221, Loss: 1.269996\n",
      "Iteration    222, Loss: 1.262589\n",
      "Iteration    223, Loss: 1.266350\n",
      "Iteration    224, Loss: 1.271868\n",
      "Iteration    225, Loss: 1.260143\n",
      "Iteration    226, Loss: 1.263493\n",
      "Iteration    227, Loss: 1.256520\n",
      "Iteration    228, Loss: 1.259067\n",
      "Iteration    229, Loss: 1.254738\n",
      "Iteration    230, Loss: 1.253381\n",
      "Iteration    231, Loss: 1.253423\n",
      "Iteration    232, Loss: 1.250927\n",
      "Iteration    233, Loss: 1.250068\n",
      "Iteration    234, Loss: 1.250919\n",
      "Iteration    235, Loss: 1.250394\n",
      "Iteration    236, Loss: 1.250095\n",
      "Iteration    237, Loss: 1.251749\n",
      "Iteration    238, Loss: 1.252152\n",
      "Iteration    239, Loss: 1.252958\n",
      "Iteration    240, Loss: 1.256820\n",
      "Iteration    241, Loss: 1.252804\n",
      "Iteration    242, Loss: 1.251217\n",
      "Iteration    243, Loss: 1.248548\n",
      "Iteration    244, Loss: 1.250135\n",
      "Iteration    245, Loss: 1.244559\n",
      "Iteration    246, Loss: 1.245211\n",
      "Iteration    247, Loss: 1.247573\n",
      "Iteration    248, Loss: 1.249986\n",
      "Iteration    249, Loss: 1.253244\n",
      "Iteration    250, Loss: 1.242532\n",
      "Iteration    251, Loss: 1.245223\n",
      "Iteration    252, Loss: 1.248183\n",
      "Iteration    253, Loss: 1.249215\n",
      "Iteration    254, Loss: 1.250315\n",
      "Iteration    255, Loss: 1.248128\n",
      "Iteration    256, Loss: 1.258618\n",
      "Iteration    257, Loss: 1.251035\n",
      "Iteration    258, Loss: 1.257812\n",
      "Iteration    259, Loss: 1.251508\n",
      "Iteration    260, Loss: 1.249616\n",
      "Iteration    261, Loss: 1.248392\n",
      "Iteration    262, Loss: 1.249087\n",
      "Iteration    263, Loss: 1.243711\n",
      "Iteration    264, Loss: 1.243140\n",
      "Iteration    265, Loss: 1.238859\n",
      "Iteration    266, Loss: 1.240612\n",
      "Iteration    267, Loss: 1.245899\n",
      "Iteration    268, Loss: 1.238624\n",
      "Iteration    269, Loss: 1.236908\n",
      "Iteration    270, Loss: 1.247340\n",
      "Iteration    271, Loss: 1.239807\n",
      "Iteration    272, Loss: 1.233809\n",
      "Iteration    273, Loss: 1.239354\n",
      "Iteration    274, Loss: 1.230043\n",
      "Iteration    275, Loss: 1.232545\n",
      "Iteration    276, Loss: 1.237168\n",
      "Iteration    277, Loss: 1.251120\n",
      "Iteration    278, Loss: 1.250750\n",
      "Iteration    279, Loss: 1.245738\n",
      "Iteration    280, Loss: 1.239664\n",
      "Iteration    281, Loss: 1.244406\n",
      "Iteration    282, Loss: 1.239282\n",
      "Iteration    283, Loss: 1.236982\n",
      "Iteration    284, Loss: 1.237425\n",
      "Iteration    285, Loss: 1.234245\n",
      "Iteration    286, Loss: 1.228652\n",
      "Iteration    287, Loss: 1.227395\n",
      "Iteration    288, Loss: 1.228188\n",
      "Iteration    289, Loss: 1.221974\n",
      "Iteration    290, Loss: 1.226749\n",
      "Iteration    291, Loss: 1.227798\n",
      "Iteration    292, Loss: 1.223845\n",
      "Iteration    293, Loss: 1.225732\n",
      "Iteration    294, Loss: 1.230761\n",
      "Iteration    295, Loss: 1.240072\n",
      "Iteration    296, Loss: 1.238478\n",
      "Iteration    297, Loss: 1.236285\n",
      "Iteration    298, Loss: 1.236716\n",
      "Iteration    299, Loss: 1.235132\n",
      "Iteration    300, Loss: 1.234774\n",
      "Iteration    301, Loss: 1.230306\n",
      "Iteration    302, Loss: 1.240852\n",
      "Iteration    303, Loss: 1.241384\n",
      "Iteration    304, Loss: 1.239826\n",
      "Iteration    305, Loss: 1.236428\n",
      "Iteration    306, Loss: 1.233467\n",
      "Iteration    307, Loss: 1.223006\n",
      "Iteration    308, Loss: 1.228613\n",
      "Iteration    309, Loss: 1.241147\n",
      "Iteration    310, Loss: 1.244168\n",
      "Iteration    311, Loss: 1.232019\n",
      "Iteration    312, Loss: 1.234596\n",
      "Iteration    313, Loss: 1.236095\n",
      "Iteration    314, Loss: 1.231593\n",
      "Iteration    315, Loss: 1.226519\n",
      "Iteration    316, Loss: 1.225810\n",
      "Iteration    317, Loss: 1.224713\n",
      "Iteration    318, Loss: 1.220166\n",
      "Iteration    319, Loss: 1.216713\n",
      "Iteration    320, Loss: 1.220604\n",
      "Iteration    321, Loss: 1.224122\n",
      "Iteration    322, Loss: 1.217710\n",
      "Iteration    323, Loss: 1.214166\n",
      "Iteration    324, Loss: 1.214984\n",
      "Iteration    325, Loss: 1.211973\n",
      "Iteration    326, Loss: 1.211781\n",
      "Iteration    327, Loss: 1.209065\n",
      "Iteration    328, Loss: 1.212749\n",
      "Iteration    329, Loss: 1.212545\n",
      "Iteration    330, Loss: 1.213781\n",
      "Iteration    331, Loss: 1.206879\n",
      "Iteration    332, Loss: 1.208511\n",
      "Iteration    333, Loss: 1.212223\n",
      "Iteration    334, Loss: 1.223784\n",
      "Iteration    335, Loss: 1.228047\n",
      "Iteration    336, Loss: 1.229195\n",
      "Iteration    337, Loss: 1.217665\n",
      "Iteration    338, Loss: 1.217347\n",
      "Iteration    339, Loss: 1.222479\n",
      "Iteration    340, Loss: 1.226838\n",
      "Iteration    341, Loss: 1.223986\n",
      "Iteration    342, Loss: 1.222618\n",
      "Iteration    343, Loss: 1.225522\n",
      "Iteration    344, Loss: 1.218645\n",
      "Iteration    345, Loss: 1.213223\n",
      "Iteration    346, Loss: 1.222995\n",
      "Iteration    347, Loss: 1.217081\n",
      "Iteration    348, Loss: 1.226858\n",
      "Iteration    349, Loss: 1.217318\n",
      "Iteration    350, Loss: 1.219611\n",
      "Iteration    351, Loss: 1.208734\n",
      "Iteration    352, Loss: 1.209003\n",
      "Iteration    353, Loss: 1.217370\n",
      "Iteration    354, Loss: 1.215237\n",
      "Iteration    355, Loss: 1.217373\n",
      "Iteration    356, Loss: 1.210631\n",
      "Iteration    357, Loss: 1.213554\n",
      "Iteration    358, Loss: 1.208020\n",
      "Iteration    359, Loss: 1.205163\n",
      "Iteration    360, Loss: 1.207999\n",
      "Iteration    361, Loss: 1.206724\n",
      "Iteration    362, Loss: 1.201568\n",
      "Iteration    363, Loss: 1.201383\n",
      "Iteration    364, Loss: 1.210992\n",
      "Iteration    365, Loss: 1.200539\n",
      "Iteration    366, Loss: 1.197707\n",
      "Iteration    367, Loss: 1.203673\n",
      "Iteration    368, Loss: 1.203247\n",
      "Iteration    369, Loss: 1.209222\n",
      "Iteration    370, Loss: 1.220035\n",
      "Iteration    371, Loss: 1.208424\n",
      "Iteration    372, Loss: 1.215483\n",
      "Iteration    373, Loss: 1.215977\n",
      "Iteration    374, Loss: 1.219073\n",
      "Iteration    375, Loss: 1.207341\n",
      "Iteration    376, Loss: 1.209686\n",
      "Iteration    377, Loss: 1.212612\n",
      "Iteration    378, Loss: 1.204252\n",
      "Iteration    379, Loss: 1.203575\n",
      "Iteration    380, Loss: 1.202479\n",
      "Iteration    381, Loss: 1.197278\n",
      "Iteration    382, Loss: 1.198269\n",
      "Iteration    383, Loss: 1.203565\n",
      "Iteration    384, Loss: 1.203433\n",
      "Iteration    385, Loss: 1.201626\n",
      "Iteration    386, Loss: 1.202500\n",
      "Iteration    387, Loss: 1.203949\n",
      "Iteration    388, Loss: 1.200461\n",
      "Iteration    389, Loss: 1.196509\n",
      "Iteration    390, Loss: 1.198403\n",
      "Iteration    391, Loss: 1.193142\n",
      "Iteration    392, Loss: 1.195979\n",
      "Iteration    393, Loss: 1.192847\n",
      "Iteration    394, Loss: 1.196651\n",
      "Iteration    395, Loss: 1.201994\n",
      "Iteration    396, Loss: 1.206504\n",
      "Iteration    397, Loss: 1.202155\n",
      "Iteration    398, Loss: 1.206096\n",
      "Iteration    399, Loss: 1.199526\n",
      "Iteration    400, Loss: 1.198936\n",
      "Iteration    401, Loss: 1.192261\n",
      "Iteration    402, Loss: 1.198257\n",
      "Iteration    403, Loss: 1.192310\n",
      "Iteration    404, Loss: 1.187314\n",
      "Iteration    405, Loss: 1.187861\n",
      "Iteration    406, Loss: 1.188748\n",
      "Iteration    407, Loss: 1.182316\n",
      "Iteration    408, Loss: 1.181358\n",
      "Iteration    409, Loss: 1.184324\n",
      "Iteration    410, Loss: 1.180606\n",
      "Iteration    411, Loss: 1.176875\n",
      "Iteration    412, Loss: 1.176831\n",
      "Iteration    413, Loss: 1.181367\n",
      "Iteration    414, Loss: 1.179128\n",
      "Iteration    415, Loss: 1.180039\n",
      "Iteration    416, Loss: 1.184304\n",
      "Iteration    417, Loss: 1.191782\n",
      "Iteration    418, Loss: 1.193556\n",
      "Iteration    419, Loss: 1.198222\n",
      "Iteration    420, Loss: 1.192284\n",
      "Iteration    421, Loss: 1.196520\n",
      "Iteration    422, Loss: 1.195414\n",
      "Iteration    423, Loss: 1.203170\n",
      "Iteration    424, Loss: 1.189275\n",
      "Iteration    425, Loss: 1.196417\n",
      "Iteration    426, Loss: 1.193252\n",
      "Iteration    427, Loss: 1.193802\n",
      "Iteration    428, Loss: 1.182735\n",
      "Iteration    429, Loss: 1.187524\n",
      "Iteration    430, Loss: 1.191814\n",
      "Iteration    431, Loss: 1.191636\n",
      "Iteration    432, Loss: 1.186196\n",
      "Iteration    433, Loss: 1.184350\n",
      "Iteration    434, Loss: 1.186763\n",
      "Iteration    435, Loss: 1.192772\n",
      "Iteration    436, Loss: 1.186324\n",
      "Iteration    437, Loss: 1.181475\n",
      "Iteration    438, Loss: 1.187311\n",
      "Iteration    439, Loss: 1.179130\n",
      "Iteration    440, Loss: 1.173820\n",
      "Iteration    441, Loss: 1.183220\n",
      "Iteration    442, Loss: 1.178636\n",
      "Iteration    443, Loss: 1.178336\n",
      "Iteration    444, Loss: 1.173942\n",
      "Iteration    445, Loss: 1.176870\n",
      "Iteration    446, Loss: 1.175943\n",
      "Iteration    447, Loss: 1.177644\n",
      "Iteration    448, Loss: 1.177535\n",
      "Iteration    449, Loss: 1.196452\n",
      "Iteration    450, Loss: 1.177025\n",
      "Iteration    451, Loss: 1.178145\n",
      "Iteration    452, Loss: 1.186460\n",
      "Iteration    453, Loss: 1.180219\n",
      "Iteration    454, Loss: 1.171708\n",
      "Iteration    455, Loss: 1.177027\n",
      "Iteration    456, Loss: 1.176563\n",
      "Iteration    457, Loss: 1.166662\n",
      "Iteration    458, Loss: 1.174842\n",
      "Iteration    459, Loss: 1.170449\n",
      "Iteration    460, Loss: 1.168518\n",
      "Iteration    461, Loss: 1.172679\n",
      "Iteration    462, Loss: 1.176421\n",
      "Iteration    463, Loss: 1.181825\n",
      "Iteration    464, Loss: 1.170784\n",
      "Iteration    465, Loss: 1.172258\n",
      "Iteration    466, Loss: 1.170270\n",
      "Iteration    467, Loss: 1.171372\n",
      "Iteration    468, Loss: 1.176870\n",
      "Iteration    469, Loss: 1.169981\n",
      "Iteration    470, Loss: 1.170334\n",
      "Iteration    471, Loss: 1.172122\n",
      "Iteration    472, Loss: 1.175795\n",
      "Iteration    473, Loss: 1.172405\n",
      "Iteration    474, Loss: 1.182357\n",
      "Iteration    475, Loss: 1.184805\n",
      "Iteration    476, Loss: 1.178002\n",
      "Iteration    477, Loss: 1.173746\n",
      "Iteration    478, Loss: 1.171311\n",
      "Iteration    479, Loss: 1.174458\n",
      "Iteration    480, Loss: 1.174352\n",
      "Iteration    481, Loss: 1.168937\n",
      "Iteration    482, Loss: 1.167910\n",
      "Iteration    483, Loss: 1.163356\n",
      "Iteration    484, Loss: 1.162211\n",
      "Iteration    485, Loss: 1.159725\n",
      "Iteration    486, Loss: 1.158538\n",
      "Iteration    487, Loss: 1.153388\n",
      "Iteration    488, Loss: 1.155770\n",
      "Iteration    489, Loss: 1.162278\n",
      "Iteration    490, Loss: 1.167003\n",
      "Iteration    491, Loss: 1.179240\n",
      "Iteration    492, Loss: 1.165480\n",
      "Iteration    493, Loss: 1.159008\n",
      "Iteration    494, Loss: 1.169235\n",
      "Iteration    495, Loss: 1.177219\n",
      "Iteration    496, Loss: 1.167836\n",
      "Iteration    497, Loss: 1.168055\n",
      "Iteration    498, Loss: 1.187661\n",
      "Iteration    499, Loss: 1.190445\n",
      "Iteration    500, Loss: 1.170776\n",
      "Iteration    501, Loss: 1.174343\n",
      "Iteration    502, Loss: 1.172729\n",
      "Iteration    503, Loss: 1.160568\n",
      "Iteration    504, Loss: 1.163872\n",
      "Iteration    505, Loss: 1.161384\n",
      "Iteration    506, Loss: 1.155252\n",
      "Iteration    507, Loss: 1.158676\n",
      "Iteration    508, Loss: 1.160912\n",
      "Iteration    509, Loss: 1.158577\n",
      "Iteration    510, Loss: 1.164113\n",
      "Iteration    511, Loss: 1.165246\n",
      "Iteration    512, Loss: 1.161766\n",
      "Iteration    513, Loss: 1.155134\n",
      "Iteration    514, Loss: 1.149639\n",
      "Iteration    515, Loss: 1.155720\n",
      "Iteration    516, Loss: 1.160585\n",
      "Iteration    517, Loss: 1.163315\n",
      "Iteration    518, Loss: 1.158476\n",
      "Iteration    519, Loss: 1.152148\n",
      "Iteration    520, Loss: 1.150434\n",
      "Iteration    521, Loss: 1.164521\n",
      "Iteration    522, Loss: 1.180534\n",
      "Iteration    523, Loss: 1.157676\n",
      "Iteration    524, Loss: 1.164235\n",
      "Iteration    525, Loss: 1.187704\n",
      "Iteration    526, Loss: 1.177079\n",
      "Iteration    527, Loss: 1.154141\n",
      "Iteration    528, Loss: 1.176191\n",
      "Iteration    529, Loss: 1.213157\n",
      "Iteration    530, Loss: 1.191709\n",
      "Iteration    531, Loss: 1.193451\n",
      "Iteration    532, Loss: 1.197371\n",
      "Iteration    533, Loss: 1.182598\n",
      "Iteration    534, Loss: 1.185493\n",
      "Iteration    535, Loss: 1.187185\n",
      "Iteration    536, Loss: 1.168485\n",
      "Iteration    537, Loss: 1.170590\n",
      "Iteration    538, Loss: 1.170145\n",
      "Iteration    539, Loss: 1.162840\n",
      "Iteration    540, Loss: 1.161020\n",
      "Iteration    541, Loss: 1.157733\n",
      "Iteration    542, Loss: 1.160232\n",
      "Iteration    543, Loss: 1.155739\n",
      "Iteration    544, Loss: 1.156618\n",
      "Iteration    545, Loss: 1.156593\n",
      "Iteration    546, Loss: 1.157202\n",
      "Iteration    547, Loss: 1.156579\n",
      "Iteration    548, Loss: 1.151641\n",
      "Iteration    549, Loss: 1.146802\n",
      "Iteration    550, Loss: 1.151937\n",
      "Iteration    551, Loss: 1.151217\n",
      "Iteration    552, Loss: 1.148649\n",
      "Iteration    553, Loss: 1.149432\n",
      "Iteration    554, Loss: 1.148500\n",
      "Iteration    555, Loss: 1.149771\n",
      "Iteration    556, Loss: 1.155618\n",
      "Iteration    557, Loss: 1.154534\n",
      "Iteration    558, Loss: 1.149924\n",
      "Iteration    559, Loss: 1.150680\n",
      "Iteration    560, Loss: 1.150259\n",
      "Iteration    561, Loss: 1.158892\n",
      "Iteration    562, Loss: 1.149901\n",
      "Iteration    563, Loss: 1.149485\n",
      "Iteration    564, Loss: 1.152763\n",
      "Iteration    565, Loss: 1.155861\n",
      "Iteration    566, Loss: 1.150158\n",
      "Iteration    567, Loss: 1.140863\n",
      "Iteration    568, Loss: 1.147351\n",
      "Iteration    569, Loss: 1.160800\n",
      "Iteration    570, Loss: 1.169638\n",
      "Iteration    571, Loss: 1.174747\n",
      "Iteration    572, Loss: 1.172696\n",
      "Iteration    573, Loss: 1.175083\n",
      "Iteration    574, Loss: 1.165117\n",
      "Iteration    575, Loss: 1.157523\n",
      "Iteration    576, Loss: 1.167242\n",
      "Iteration    577, Loss: 1.159057\n",
      "Iteration    578, Loss: 1.152333\n",
      "Iteration    579, Loss: 1.152505\n",
      "Iteration    580, Loss: 1.143866\n",
      "Iteration    581, Loss: 1.144288\n",
      "Iteration    582, Loss: 1.145807\n",
      "Iteration    583, Loss: 1.145712\n",
      "Iteration    584, Loss: 1.152637\n",
      "Iteration    585, Loss: 1.170716\n",
      "Iteration    586, Loss: 1.185655\n",
      "Iteration    587, Loss: 1.160133\n",
      "Iteration    588, Loss: 1.187320\n",
      "Iteration    589, Loss: 1.212747\n",
      "Iteration    590, Loss: 1.165122\n",
      "Iteration    591, Loss: 1.187265\n",
      "Iteration    592, Loss: 1.186417\n",
      "Iteration    593, Loss: 1.156595\n",
      "Iteration    594, Loss: 1.172594\n",
      "Iteration    595, Loss: 1.157979\n",
      "Iteration    596, Loss: 1.158343\n",
      "Iteration    597, Loss: 1.158974\n",
      "Iteration    598, Loss: 1.139012\n",
      "Iteration    599, Loss: 1.147678\n",
      "Iteration    600, Loss: 1.144767\n",
      "Iteration    601, Loss: 1.136308\n",
      "Iteration    602, Loss: 1.139111\n",
      "Iteration    603, Loss: 1.140688\n",
      "Iteration    604, Loss: 1.128207\n",
      "Iteration    605, Loss: 1.134379\n",
      "Iteration    606, Loss: 1.132736\n",
      "Iteration    607, Loss: 1.128575\n",
      "Iteration    608, Loss: 1.130587\n",
      "Iteration    609, Loss: 1.131020\n",
      "Iteration    610, Loss: 1.125983\n",
      "Iteration    611, Loss: 1.125306\n",
      "Iteration    612, Loss: 1.124000\n",
      "Iteration    613, Loss: 1.120659\n",
      "Iteration    614, Loss: 1.121924\n",
      "Iteration    615, Loss: 1.118659\n",
      "Iteration    616, Loss: 1.122908\n",
      "Iteration    617, Loss: 1.125016\n",
      "Iteration    618, Loss: 1.139290\n",
      "Iteration    619, Loss: 1.144931\n",
      "Iteration    620, Loss: 1.158800\n",
      "Iteration    621, Loss: 1.158394\n",
      "Iteration    622, Loss: 1.154504\n",
      "Iteration    623, Loss: 1.151187\n",
      "Iteration    624, Loss: 1.146572\n",
      "Iteration    625, Loss: 1.146556\n",
      "Iteration    626, Loss: 1.142587\n",
      "Iteration    627, Loss: 1.138417\n",
      "Iteration    628, Loss: 1.131091\n",
      "Iteration    629, Loss: 1.137904\n",
      "Iteration    630, Loss: 1.134882\n",
      "Iteration    631, Loss: 1.127581\n",
      "Iteration    632, Loss: 1.128788\n",
      "Iteration    633, Loss: 1.128441\n",
      "Iteration    634, Loss: 1.128907\n",
      "Iteration    635, Loss: 1.130697\n",
      "Iteration    636, Loss: 1.130658\n",
      "Iteration    637, Loss: 1.133735\n",
      "Iteration    638, Loss: 1.131611\n",
      "Iteration    639, Loss: 1.128040\n",
      "Iteration    640, Loss: 1.140480\n",
      "Iteration    641, Loss: 1.138003\n",
      "Iteration    642, Loss: 1.130100\n",
      "Iteration    643, Loss: 1.121497\n",
      "Iteration    644, Loss: 1.121194\n",
      "Iteration    645, Loss: 1.128777\n",
      "Iteration    646, Loss: 1.128105\n",
      "Iteration    647, Loss: 1.126356\n",
      "Iteration    648, Loss: 1.120665\n",
      "Iteration    649, Loss: 1.120783\n",
      "Iteration    650, Loss: 1.130784\n",
      "Iteration    651, Loss: 1.122900\n",
      "Iteration    652, Loss: 1.118306\n",
      "Iteration    653, Loss: 1.117434\n",
      "Iteration    654, Loss: 1.115590\n",
      "Iteration    655, Loss: 1.114954\n",
      "Iteration    656, Loss: 1.123706\n",
      "Iteration    657, Loss: 1.136972\n",
      "Iteration    658, Loss: 1.133455\n",
      "Iteration    659, Loss: 1.126071\n",
      "Iteration    660, Loss: 1.132312\n",
      "Iteration    661, Loss: 1.137884\n",
      "Iteration    662, Loss: 1.125786\n",
      "Iteration    663, Loss: 1.122432\n",
      "Iteration    664, Loss: 1.136064\n",
      "Iteration    665, Loss: 1.140352\n",
      "Iteration    666, Loss: 1.145029\n",
      "Iteration    667, Loss: 1.129747\n",
      "Iteration    668, Loss: 1.143340\n",
      "Iteration    669, Loss: 1.149954\n",
      "Iteration    670, Loss: 1.158267\n",
      "Iteration    671, Loss: 1.141503\n",
      "Iteration    672, Loss: 1.147633\n",
      "Iteration    673, Loss: 1.157238\n",
      "Iteration    674, Loss: 1.156495\n",
      "Iteration    675, Loss: 1.133827\n",
      "Iteration    676, Loss: 1.141871\n",
      "Iteration    677, Loss: 1.138099\n",
      "Iteration    678, Loss: 1.127708\n",
      "Iteration    679, Loss: 1.124518\n",
      "Iteration    680, Loss: 1.129700\n",
      "Iteration    681, Loss: 1.139488\n",
      "Iteration    682, Loss: 1.126208\n",
      "Iteration    683, Loss: 1.122898\n",
      "Iteration    684, Loss: 1.128716\n",
      "Iteration    685, Loss: 1.126314\n",
      "Iteration    686, Loss: 1.127158\n",
      "Iteration    687, Loss: 1.127150\n",
      "Iteration    688, Loss: 1.118302\n",
      "Iteration    689, Loss: 1.117279\n",
      "Iteration    690, Loss: 1.118623\n",
      "Iteration    691, Loss: 1.116690\n",
      "Iteration    692, Loss: 1.115869\n",
      "Iteration    693, Loss: 1.120962\n",
      "Iteration    694, Loss: 1.111300\n",
      "Iteration    695, Loss: 1.106663\n",
      "Iteration    696, Loss: 1.109929\n",
      "Iteration    697, Loss: 1.111418\n",
      "Iteration    698, Loss: 1.105685\n",
      "Iteration    699, Loss: 1.100777\n",
      "Iteration    700, Loss: 1.104563\n",
      "Iteration    701, Loss: 1.110770\n",
      "Iteration    702, Loss: 1.112205\n",
      "Iteration    703, Loss: 1.114352\n",
      "Iteration    704, Loss: 1.115270\n",
      "Iteration    705, Loss: 1.116631\n",
      "Iteration    706, Loss: 1.101766\n",
      "Iteration    707, Loss: 1.105161\n",
      "Iteration    708, Loss: 1.115650\n",
      "Iteration    709, Loss: 1.120036\n",
      "Iteration    710, Loss: 1.115485\n",
      "Iteration    711, Loss: 1.117460\n",
      "Iteration    712, Loss: 1.131006\n",
      "Iteration    713, Loss: 1.120402\n",
      "Iteration    714, Loss: 1.135335\n",
      "Iteration    715, Loss: 1.153356\n",
      "Iteration    716, Loss: 1.141621\n",
      "Iteration    717, Loss: 1.151245\n",
      "Iteration    718, Loss: 1.148914\n",
      "Iteration    719, Loss: 1.120760\n",
      "Iteration    720, Loss: 1.141944\n",
      "Iteration    721, Loss: 1.131124\n",
      "Iteration    722, Loss: 1.111346\n",
      "Iteration    723, Loss: 1.141778\n",
      "Iteration    724, Loss: 1.132425\n",
      "Iteration    725, Loss: 1.117652\n",
      "Iteration    726, Loss: 1.133521\n",
      "Iteration    727, Loss: 1.124386\n",
      "Iteration    728, Loss: 1.110296\n",
      "Iteration    729, Loss: 1.128563\n",
      "Iteration    730, Loss: 1.117358\n",
      "Iteration    731, Loss: 1.103566\n",
      "Iteration    732, Loss: 1.118474\n",
      "Iteration    733, Loss: 1.115820\n",
      "Iteration    734, Loss: 1.111351\n",
      "Iteration    735, Loss: 1.111398\n",
      "Iteration    736, Loss: 1.113559\n",
      "Iteration    737, Loss: 1.115249\n",
      "Iteration    738, Loss: 1.117040\n",
      "Iteration    739, Loss: 1.111320\n",
      "Iteration    740, Loss: 1.103387\n",
      "Iteration    741, Loss: 1.111078\n",
      "Iteration    742, Loss: 1.095622\n",
      "Iteration    743, Loss: 1.101338\n",
      "Iteration    744, Loss: 1.101982\n",
      "Iteration    745, Loss: 1.102283\n",
      "Iteration    746, Loss: 1.098616\n",
      "Iteration    747, Loss: 1.107539\n",
      "Iteration    748, Loss: 1.110160\n",
      "Iteration    749, Loss: 1.101902\n",
      "Iteration    750, Loss: 1.093368\n",
      "Iteration    751, Loss: 1.092722\n",
      "Iteration    752, Loss: 1.093782\n",
      "Iteration    753, Loss: 1.101224\n",
      "Iteration    754, Loss: 1.105179\n",
      "Iteration    755, Loss: 1.095134\n",
      "Iteration    756, Loss: 1.097631\n",
      "Iteration    757, Loss: 1.115195\n",
      "Iteration    758, Loss: 1.121439\n",
      "Iteration    759, Loss: 1.120279\n",
      "Iteration    760, Loss: 1.117547\n",
      "Iteration    761, Loss: 1.109732\n",
      "Iteration    762, Loss: 1.127340\n",
      "Iteration    763, Loss: 1.145465\n",
      "Iteration    764, Loss: 1.114497\n",
      "Iteration    765, Loss: 1.120970\n",
      "Iteration    766, Loss: 1.126391\n",
      "Iteration    767, Loss: 1.114116\n",
      "Iteration    768, Loss: 1.101339\n",
      "Iteration    769, Loss: 1.115515\n",
      "Iteration    770, Loss: 1.108582\n",
      "Iteration    771, Loss: 1.092425\n",
      "Iteration    772, Loss: 1.103413\n",
      "Iteration    773, Loss: 1.109233\n",
      "Iteration    774, Loss: 1.101752\n",
      "Iteration    775, Loss: 1.103228\n",
      "Iteration    776, Loss: 1.106670\n",
      "Iteration    777, Loss: 1.094439\n",
      "Iteration    778, Loss: 1.098282\n",
      "Iteration    779, Loss: 1.097881\n",
      "Iteration    780, Loss: 1.093769\n",
      "Iteration    781, Loss: 1.090210\n",
      "Iteration    782, Loss: 1.098320\n",
      "Iteration    783, Loss: 1.107797\n",
      "Iteration    784, Loss: 1.093709\n",
      "Iteration    785, Loss: 1.098467\n",
      "Iteration    786, Loss: 1.110142\n",
      "Iteration    787, Loss: 1.116596\n",
      "Iteration    788, Loss: 1.096490\n",
      "Iteration    789, Loss: 1.090159\n",
      "Iteration    790, Loss: 1.117527\n",
      "Iteration    791, Loss: 1.127753\n",
      "Iteration    792, Loss: 1.113550\n",
      "Iteration    793, Loss: 1.117566\n",
      "Iteration    794, Loss: 1.126641\n",
      "Iteration    795, Loss: 1.114387\n",
      "Iteration    796, Loss: 1.115727\n",
      "Iteration    797, Loss: 1.115901\n",
      "Iteration    798, Loss: 1.106723\n",
      "Iteration    799, Loss: 1.113898\n",
      "Iteration    800, Loss: 1.122559\n",
      "Iteration    801, Loss: 1.108979\n",
      "Iteration    802, Loss: 1.107002\n",
      "Iteration    803, Loss: 1.131166\n",
      "Iteration    804, Loss: 1.112975\n",
      "Iteration    805, Loss: 1.106973\n",
      "Iteration    806, Loss: 1.109794\n",
      "Iteration    807, Loss: 1.107201\n",
      "Iteration    808, Loss: 1.105998\n",
      "Iteration    809, Loss: 1.101728\n",
      "Iteration    810, Loss: 1.094060\n",
      "Iteration    811, Loss: 1.090496\n",
      "Iteration    812, Loss: 1.100083\n",
      "Iteration    813, Loss: 1.096460\n",
      "Iteration    814, Loss: 1.090988\n",
      "Iteration    815, Loss: 1.093361\n",
      "Iteration    816, Loss: 1.092176\n",
      "Iteration    817, Loss: 1.086691\n",
      "Iteration    818, Loss: 1.092596\n",
      "Iteration    819, Loss: 1.091702\n",
      "Iteration    820, Loss: 1.078791\n",
      "Iteration    821, Loss: 1.082294\n",
      "Iteration    822, Loss: 1.088283\n",
      "Iteration    823, Loss: 1.093625\n",
      "Iteration    824, Loss: 1.081685\n",
      "Iteration    825, Loss: 1.086498\n",
      "Iteration    826, Loss: 1.095052\n",
      "Iteration    827, Loss: 1.097253\n",
      "Iteration    828, Loss: 1.091703\n",
      "Iteration    829, Loss: 1.087636\n",
      "Iteration    830, Loss: 1.088882\n",
      "Iteration    831, Loss: 1.084170\n",
      "Iteration    832, Loss: 1.089716\n",
      "Iteration    833, Loss: 1.097743\n",
      "Iteration    834, Loss: 1.083417\n",
      "Iteration    835, Loss: 1.080884\n",
      "Iteration    836, Loss: 1.092909\n",
      "Iteration    837, Loss: 1.104190\n",
      "Iteration    838, Loss: 1.091327\n",
      "Iteration    839, Loss: 1.078993\n",
      "Iteration    840, Loss: 1.097270\n",
      "Iteration    841, Loss: 1.106243\n",
      "Iteration    842, Loss: 1.089741\n",
      "Iteration    843, Loss: 1.075962\n",
      "Iteration    844, Loss: 1.105032\n",
      "Iteration    845, Loss: 1.085837\n",
      "Iteration    846, Loss: 1.080290\n",
      "Iteration    847, Loss: 1.082473\n",
      "Iteration    848, Loss: 1.095829\n",
      "Iteration    849, Loss: 1.091103\n",
      "Iteration    850, Loss: 1.085080\n",
      "Iteration    851, Loss: 1.083082\n",
      "Iteration    852, Loss: 1.088993\n",
      "Iteration    853, Loss: 1.082803\n",
      "Iteration    854, Loss: 1.074989\n",
      "Iteration    855, Loss: 1.077850\n",
      "Iteration    856, Loss: 1.090192\n",
      "Iteration    857, Loss: 1.084892\n",
      "Iteration    858, Loss: 1.070137\n",
      "Iteration    859, Loss: 1.079950\n",
      "Iteration    860, Loss: 1.074640\n",
      "Iteration    861, Loss: 1.078003\n",
      "Iteration    862, Loss: 1.075260\n",
      "Iteration    863, Loss: 1.081716\n",
      "Iteration    864, Loss: 1.093459\n",
      "Iteration    865, Loss: 1.102667\n",
      "Iteration    866, Loss: 1.095975\n",
      "Iteration    867, Loss: 1.094805\n",
      "Iteration    868, Loss: 1.102053\n",
      "Iteration    869, Loss: 1.105494\n",
      "Iteration    870, Loss: 1.093551\n",
      "Iteration    871, Loss: 1.104375\n",
      "Iteration    872, Loss: 1.106471\n",
      "Iteration    873, Loss: 1.094852\n",
      "Iteration    874, Loss: 1.096715\n",
      "Iteration    875, Loss: 1.103503\n",
      "Iteration    876, Loss: 1.090664\n",
      "Iteration    877, Loss: 1.088073\n",
      "Iteration    878, Loss: 1.094867\n",
      "Iteration    879, Loss: 1.085957\n",
      "Iteration    880, Loss: 1.080008\n",
      "Iteration    881, Loss: 1.085648\n",
      "Iteration    882, Loss: 1.092274\n",
      "Iteration    883, Loss: 1.086481\n",
      "Iteration    884, Loss: 1.087243\n",
      "Iteration    885, Loss: 1.081552\n",
      "Iteration    886, Loss: 1.079125\n",
      "Iteration    887, Loss: 1.072982\n",
      "Iteration    888, Loss: 1.070414\n",
      "Iteration    889, Loss: 1.073890\n",
      "Iteration    890, Loss: 1.077990\n",
      "Iteration    891, Loss: 1.071190\n",
      "Iteration    892, Loss: 1.078503\n",
      "Iteration    893, Loss: 1.071124\n",
      "Iteration    894, Loss: 1.073990\n",
      "Iteration    895, Loss: 1.072150\n",
      "Iteration    896, Loss: 1.064453\n",
      "Iteration    897, Loss: 1.068250\n",
      "Iteration    898, Loss: 1.073443\n",
      "Iteration    899, Loss: 1.068708\n",
      "Iteration    900, Loss: 1.068820\n",
      "Iteration    901, Loss: 1.070361\n",
      "Iteration    902, Loss: 1.073188\n",
      "Iteration    903, Loss: 1.060271\n",
      "Iteration    904, Loss: 1.061675\n",
      "Iteration    905, Loss: 1.064081\n",
      "Iteration    906, Loss: 1.064949\n",
      "Iteration    907, Loss: 1.066864\n",
      "Iteration    908, Loss: 1.062635\n",
      "Iteration    909, Loss: 1.063478\n",
      "Iteration    910, Loss: 1.062271\n",
      "Iteration    911, Loss: 1.055012\n",
      "Iteration    912, Loss: 1.056460\n",
      "Iteration    913, Loss: 1.061396\n",
      "Iteration    914, Loss: 1.072192\n",
      "Iteration    915, Loss: 1.083732\n",
      "Iteration    916, Loss: 1.089016\n",
      "Iteration    917, Loss: 1.091769\n",
      "Iteration    918, Loss: 1.086636\n",
      "Iteration    919, Loss: 1.094816\n",
      "Iteration    920, Loss: 1.107140\n",
      "Iteration    921, Loss: 1.086105\n",
      "Iteration    922, Loss: 1.085887\n",
      "Iteration    923, Loss: 1.099885\n",
      "Iteration    924, Loss: 1.100859\n",
      "Iteration    925, Loss: 1.092605\n",
      "Iteration    926, Loss: 1.095493\n",
      "Iteration    927, Loss: 1.077517\n",
      "Iteration    928, Loss: 1.073438\n",
      "Iteration    929, Loss: 1.085966\n",
      "Iteration    930, Loss: 1.081184\n",
      "Iteration    931, Loss: 1.077628\n",
      "Iteration    932, Loss: 1.080726\n",
      "Iteration    933, Loss: 1.088035\n",
      "Iteration    934, Loss: 1.098988\n",
      "Iteration    935, Loss: 1.106810\n",
      "Iteration    936, Loss: 1.084985\n",
      "Iteration    937, Loss: 1.096887\n",
      "Iteration    938, Loss: 1.099773\n",
      "Iteration    939, Loss: 1.090401\n",
      "Iteration    940, Loss: 1.084743\n",
      "Iteration    941, Loss: 1.082938\n",
      "Iteration    942, Loss: 1.083139\n",
      "Iteration    943, Loss: 1.081825\n",
      "Iteration    944, Loss: 1.069284\n",
      "Iteration    945, Loss: 1.077412\n",
      "Iteration    946, Loss: 1.078616\n",
      "Iteration    947, Loss: 1.078127\n",
      "Iteration    948, Loss: 1.072501\n",
      "Iteration    949, Loss: 1.066909\n",
      "Iteration    950, Loss: 1.075306\n",
      "Iteration    951, Loss: 1.080065\n",
      "Iteration    952, Loss: 1.087139\n",
      "Iteration    953, Loss: 1.078326\n",
      "Iteration    954, Loss: 1.061479\n",
      "Iteration    955, Loss: 1.069256\n",
      "Iteration    956, Loss: 1.071277\n",
      "Iteration    957, Loss: 1.080257\n",
      "Iteration    958, Loss: 1.071782\n",
      "Iteration    959, Loss: 1.064381\n",
      "Iteration    960, Loss: 1.071955\n",
      "Iteration    961, Loss: 1.070963\n",
      "Iteration    962, Loss: 1.076494\n",
      "Iteration    963, Loss: 1.064965\n",
      "Iteration    964, Loss: 1.056898\n",
      "Iteration    965, Loss: 1.062431\n",
      "Iteration    966, Loss: 1.078251\n",
      "Iteration    967, Loss: 1.081771\n",
      "Iteration    968, Loss: 1.072725\n",
      "Iteration    969, Loss: 1.071194\n",
      "Iteration    970, Loss: 1.082872\n",
      "Iteration    971, Loss: 1.084697\n",
      "Iteration    972, Loss: 1.075046\n",
      "Iteration    973, Loss: 1.073352\n",
      "Iteration    974, Loss: 1.078984\n",
      "Iteration    975, Loss: 1.063872\n",
      "Iteration    976, Loss: 1.070143\n",
      "Iteration    977, Loss: 1.074646\n",
      "Iteration    978, Loss: 1.068978\n",
      "Iteration    979, Loss: 1.072480\n",
      "Iteration    980, Loss: 1.084104\n",
      "Iteration    981, Loss: 1.085511\n",
      "Iteration    982, Loss: 1.060752\n",
      "Iteration    983, Loss: 1.074881\n",
      "Iteration    984, Loss: 1.105566\n",
      "Iteration    985, Loss: 1.072720\n",
      "Iteration    986, Loss: 1.065013\n",
      "Iteration    987, Loss: 1.086017\n",
      "Iteration    988, Loss: 1.079015\n",
      "Iteration    989, Loss: 1.069012\n",
      "Iteration    990, Loss: 1.064166\n",
      "Iteration    991, Loss: 1.078575\n",
      "Iteration    992, Loss: 1.071552\n",
      "Iteration    993, Loss: 1.062365\n",
      "Iteration    994, Loss: 1.058167\n",
      "Iteration    995, Loss: 1.070374\n",
      "Iteration    996, Loss: 1.071773\n",
      "Iteration    997, Loss: 1.068612\n",
      "Iteration    998, Loss: 1.051474\n",
      "Iteration    999, Loss: 1.062188\n",
      "Iteration   1000, Loss: 1.056664\n"
     ]
    }
   ],
   "source": [
    "steps = 1000\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for i in range(steps):\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    print(f\"Iteration {i + 1:6d}, Loss: {loss.item():6f}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
