{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "import face_recognition\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "from imutils import face_utils\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'EngageNet/'\n",
    "DATASET = 'Train'\n",
    "\n",
    "FPS = 30\n",
    "BATCH_SIZE = 1\n",
    "VIDEO_LENGTH = 10\n",
    "FRAME_INTERVAL = 2\n",
    "GLOBAL_BATCH_SIZE = 7983\n",
    "\n",
    "def get_subdirectories(video_paths, sort=False):\n",
    "    \"\"\"Get a list of subject-wise videos.\"\"\"\n",
    "    sub_directories = {}\n",
    "\n",
    "    for path in video_paths:\n",
    "        prefix = ''.join(path.split('_')[:-2])\n",
    "\n",
    "        if prefix not in sub_directories:\n",
    "            sub_directories[prefix] = []\n",
    "        \n",
    "        sub_directories[prefix].append(path)\n",
    "\n",
    "    sub_directories = list(sub_directories.values())\n",
    "\n",
    "    if sort:\n",
    "        sub_directories.sort()\n",
    "    else:\n",
    "        random.shuffle(sub_directories)\n",
    "    \n",
    "    return sub_directories\n",
    "\n",
    "def get_videos(path, sort=False):\n",
    "    \"\"\"Get a list of video paths from a directory 'path'.\"\"\"\n",
    "    videos = [os.path.join(path, file) for file in os.listdir(path) if file.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    \n",
    "    if sort:\n",
    "        videos.sort()\n",
    "    else:\n",
    "        random.shuffle(videos)\n",
    "    \n",
    "    return videos\n",
    "\n",
    "def get_video_paths(path, sort=False):\n",
    "    \"\"\"Get all video paths in a dataset.\"\"\"\n",
    "    videos = []\n",
    "    videos = get_videos(path, sort)\n",
    "    subject_list = get_subdirectories(videos)\n",
    "    videos = [video for subject_videos in subject_list for video in subject_videos]\n",
    "    return videos\n",
    "\n",
    "def get_frames(subject_videos, frame_interval=FRAME_INTERVAL, resize_to=None):\n",
    "    \"\"\"Get frames from a list of 'subject_videos' paths.\"\"\"\n",
    "    frames_subject = []\n",
    "\n",
    "    for subject_video in subject_videos:\n",
    "        video_capture = cv2.VideoCapture(subject_video)\n",
    "        \n",
    "        if not video_capture.isOpened():\n",
    "            print(f\"Error opening video file {subject_video}\")\n",
    "            continue\n",
    "\n",
    "        count = 0\n",
    "        frames = []\n",
    "\n",
    "        while True:\n",
    "            success, frame = video_capture.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            count += 1\n",
    "            if count % frame_interval == 0:\n",
    "                if resize_to:\n",
    "                    frame = cv2.resize(frame, resize_to)\n",
    "\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "\n",
    "        video_capture.release()\n",
    "\n",
    "        if frames:\n",
    "            frames_required = (FPS * VIDEO_LENGTH) // frame_interval\n",
    "            frames = frames[:frames_required]\n",
    "            while len(frames) < frames_required:\n",
    "                frames.append(frames[-1])\n",
    "            frames_subject.append(frames)\n",
    "        else:\n",
    "            print(f\"No frames extracted from video file {subject_video}\")\n",
    "\n",
    "    return frames_subject if frames_subject else [[]]\n",
    "\n",
    "def get_labels(paths):\n",
    "    \"\"\"Get labels for engagement.\"\"\"\n",
    "    stoi = {\n",
    "        'SNP(Subject Not Present)': 0,\n",
    "        'Not-Engaged' : 0,\n",
    "        'Barely-engaged': 1,\n",
    "        'Engaged': 2,\n",
    "        'Highly-Engaged': 3\n",
    "    }\n",
    "\n",
    "    data = pd.read_csv(f'EngageNet/{DATASET.lower()}_engagement_levels.csv')\n",
    "    tails = [os.path.split(path)[-1] for path in paths]\n",
    "    filtered_data = data[data['chunk'].isin(tails)]\n",
    "    engagement_data = filtered_data['label'].map(stoi)\n",
    "    return engagement_data.values\n",
    "\n",
    "iterator = 0\n",
    "def load_data(path, dataset, batch_size):\n",
    "    \"\"\"Load random videos from 'path'.\"\"\"\n",
    "    global iterator\n",
    "    path = os.path.join(path, dataset)\n",
    "    paths = get_video_paths(path, sort=True)\n",
    "    X = get_frames(paths[iterator:batch_size+iterator], FRAME_INTERVAL)\n",
    "    Y = get_labels(paths[iterator:batch_size+iterator])\n",
    "    \n",
    "    iterator += batch_size\n",
    "    if iterator >= len(paths):\n",
    "        iterator = 0\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_predictor_path = \"eameo-faceswap-generator/shape_predictor_68_face_landmarks.dat\"\n",
    "try:\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "    print(\"Successfully loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    exit()\n",
    "\n",
    "def eye_aspect_ratio_torch(eye):\n",
    "    A = torch.norm(eye[1] - eye[5])\n",
    "    B = torch.norm(eye[2] - eye[4])\n",
    "    C = torch.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def get_head_pose_torch(shape, size, device='cpu'):\n",
    "    model_points = torch.tensor([\n",
    "        [0.0, 0.0, 0.0], [0.0, -330.0, -65.0], [-225.0, 170.0, -135.0],\n",
    "        [225.0, 170.0, -135.0], [-150.0, -150.0, -125.0], [150.0, -150.0, -125.0]\n",
    "    ], dtype=torch.float64).to(device)\n",
    "\n",
    "    image_points = shape[[30, 8, 36, 45, 48, 54], :]\n",
    "\n",
    "    if image_points.shape[0] < 4:\n",
    "        raise ValueError(\"Not enough points to perform solvePnP\")\n",
    "\n",
    "    focal_length = size[1]\n",
    "    center = (size[1] / 2, size[0] / 2)\n",
    "    camera_matrix = torch.tensor([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=torch.float64).to(device)\n",
    "\n",
    "    dist_coeffs = torch.zeros((4, 1), dtype=torch.float64).to(device)\n",
    "    \n",
    "    image_points_np = image_points.cpu().numpy()\n",
    "    model_points_np = model_points.cpu().numpy()\n",
    "    camera_matrix_np = camera_matrix.cpu().numpy()\n",
    "    dist_coeffs_np = dist_coeffs.cpu().numpy()\n",
    "\n",
    "    success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "        model_points_np, image_points_np, camera_matrix_np, dist_coeffs_np\n",
    "    )\n",
    "\n",
    "    if not success:\n",
    "        raise ValueError(\"solvePnP failed to find a solution\")\n",
    "\n",
    "    rotation_vector = torch.tensor(rotation_vector, dtype=torch.float64).to(device)\n",
    "    translation_vector = torch.tensor(translation_vector, dtype=torch.float64).to(device)\n",
    "    \n",
    "    return rotation_vector, translation_vector\n",
    "\n",
    "def rotation_vector_to_euler_angles(rotation_vector):\n",
    "    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "    sy = np.sqrt(rotation_matrix[0, 0] ** 2 + rotation_matrix[1, 0] ** 2)\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if not singular:\n",
    "        x = np.arctan2(rotation_matrix[2, 1], rotation_matrix[2, 2])\n",
    "        y = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "        z = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
    "    else:\n",
    "        x = np.arctan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])\n",
    "        y = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.degrees(x), np.degrees(y), np.degrees(z)\n",
    "\n",
    "def pad_features(features, target_length):\n",
    "    padded_features = []\n",
    "    for feature_vector in features:\n",
    "        if isinstance(feature_vector, tuple):\n",
    "            feature_vector = list(feature_vector)\n",
    "        if len(feature_vector) < target_length:\n",
    "            padded_vector = feature_vector + [0] * (target_length - len(feature_vector))\n",
    "        else:\n",
    "            padded_vector = feature_vector[:target_length]\n",
    "        padded_features.append(padded_vector)\n",
    "    return padded_features\n",
    "\n",
    "def classify_eye_openness(ear, threshold_open=0.3, threshold_closed=0.2):\n",
    "    \"\"\"\n",
    "    Classifies eye openness based on EAR (Eye Aspect Ratio).\n",
    "\n",
    "    Parameters:\n",
    "    - ear (float): The EAR value for the current frame.\n",
    "    - threshold_open (float): The EAR threshold above which eyes are considered fully open.\n",
    "    - threshold_closed (float): The EAR threshold below which eyes are considered closed.\n",
    "\n",
    "    Returns:\n",
    "    - str: A classification of the eye openness (\"Fully Open\", \"Partially Open\", \"Closed\").\n",
    "    \"\"\"\n",
    "    if ear > threshold_open:\n",
    "        return 0\n",
    "    elif threshold_closed <= ear <= threshold_open:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def compute_gaze_direction(shape_tensor, rotation_vector, image_size):\n",
    "    \"\"\"\n",
    "    Compute gaze direction based on eye landmarks and head pose.\n",
    "    \n",
    "    Parameters:\n",
    "        shape_tensor (torch.Tensor): Landmarks of the face.\n",
    "        rotation_vector (torch.Tensor): Rotation vector from head pose estimation.\n",
    "        image_size (tuple): Size of the image (height, width).\n",
    "    \n",
    "    Returns:\n",
    "        gaze_direction (tuple): Estimated gaze direction (pitch, yaw).\n",
    "    \"\"\"\n",
    "    left_eye_landmarks = shape_tensor[36:42]\n",
    "    right_eye_landmarks = shape_tensor[42:48]\n",
    "    \n",
    "    left_eye_center = left_eye_landmarks.mean(dim=0)\n",
    "    right_eye_center = right_eye_landmarks.mean(dim=0)\n",
    "    \n",
    "    eye_center = (left_eye_center + right_eye_center) / 2\n",
    "    \n",
    "    image_center = torch.tensor([image_size[1] / 2, image_size[0] / 2], dtype=torch.float32)\n",
    "    \n",
    "    gaze_vector = image_center - eye_center[:2]\n",
    "    \n",
    "    pitch, yaw, roll = rotation_vector_to_euler_angles(rotation_vector.cpu().numpy())\n",
    "    \n",
    "    return pitch, yaw, gaze_vector.tolist()\n",
    "\n",
    "def classify_head_pose(pitch, yaw, roll, gaze_vector):\n",
    "    \"\"\"\n",
    "    Classifies head pose based on pitch, yaw, roll, and gaze direction.\n",
    "    \n",
    "    Parameters:\n",
    "        pitch (float): The pitch angle.\n",
    "        yaw (float): The yaw angle.\n",
    "        roll (float): The roll angle.\n",
    "        gaze_vector (list): The gaze direction vector [x, y].\n",
    "    \n",
    "    Returns:\n",
    "        str: A classification string representing the head pose.\n",
    "    \"\"\"\n",
    "    gaze_direction = 0  \n",
    "    if gaze_vector[0] > 0.5:\n",
    "        gaze_direction = 1 \n",
    "    elif gaze_vector[0] < -0.5:\n",
    "        gaze_direction = 2 \n",
    "    \n",
    "    if pitch > 10:\n",
    "        position = 3 \n",
    "    elif pitch < -10:\n",
    "        position = 4\n",
    "    elif yaw > 10:\n",
    "        position = 5 \n",
    "    elif yaw < -10:\n",
    "        position = 6 \n",
    "    elif roll > 10:\n",
    "        position = 7\n",
    "    elif roll < -10:\n",
    "        position = 8 \n",
    "    else:\n",
    "        position = 0\n",
    "    \n",
    "    return position, gaze_direction\n",
    "\n",
    "def process_frame(frame):\n",
    "    frame_np = frame\n",
    "    gray = cv2.cvtColor(frame_np, cv2.COLOR_RGB2GRAY)\n",
    "    rects = detector(gray, 0)\n",
    "    features = []\n",
    "\n",
    "    shapes = [face_utils.shape_to_np(predictor(gray, rect)) for rect in rects]\n",
    "    best_face_index = None\n",
    "    if len(rects) == 0:\n",
    "        return features\n",
    "    max_width = 0\n",
    "    best_features = None\n",
    "    rects = sorted(rects, key=lambda rect: rect.width() * rect.height(), reverse=True)\n",
    "    MIN_FACE_SIZE = 5000\n",
    "    for i, rect in enumerate(rects):\n",
    "        if rect.width() * rect.height() < MIN_FACE_SIZE:\n",
    "            continue \n",
    "        shape = shapes[i]\n",
    "        shape_tensor = torch.tensor(shape, dtype=torch.float32)\n",
    "\n",
    "        leftEye = shape_tensor[42:48]\n",
    "        rightEye = shape_tensor[36:42]\n",
    "        ear = (eye_aspect_ratio_torch(leftEye) + eye_aspect_ratio_torch(rightEye)) / 2.0\n",
    "\n",
    "        size = (frame_np.shape[1], frame_np.shape[0])\n",
    "        rotation_vector, translation_vector = get_head_pose_torch(shape_tensor, size)\n",
    "        pitch, yaw, roll = rotation_vector_to_euler_angles(rotation_vector.numpy())\n",
    "\n",
    "        if pitch > 90:\n",
    "            if pitch < 0:\n",
    "                pitch = abs(pitch)-180\n",
    "                pitch = 0-pitch\n",
    "            else:\n",
    "                pitch = abs(pitch)-190\n",
    "        gaze_vector = compute_gaze_direction(shape_tensor, rotation_vector, size)\n",
    "        position, gaze_direction = classify_head_pose(pitch, yaw, roll, gaze_vector)\n",
    "        eye_category = classify_eye_openness(ear.item())\n",
    "\n",
    "        current_features = [eye_category, position, gaze_direction]\n",
    "\n",
    "        face_width = rect.width() * rect.height()\n",
    "        if face_width > max_width:\n",
    "            max_width = face_width\n",
    "            best_face_index = i\n",
    "            best_features = current_features\n",
    "\n",
    "    if best_face_index is not None:\n",
    "        features = best_features\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend(input_signal, lambda_value):\n",
    "    signal_length = input_signal.shape[0]\n",
    "\n",
    "    H = np.identity(signal_length)\n",
    "    ones = np.ones(signal_length)\n",
    "    minus_twos = -2 * np.ones(signal_length)\n",
    "    diags_data = np.array([ones, minus_twos, ones])\n",
    "    diags_index = np.array([0, 1, 2])\n",
    "    D = sp.sparse.spdiags(diags_data, diags_index,\n",
    "                (signal_length - 2), signal_length).toarray()\n",
    "    filtered_signal = np.dot(\n",
    "        (H - np.linalg.inv(H + (lambda_value ** 2) * np.dot(D.T, D))), input_signal)\n",
    "    return filtered_signal\n",
    "\n",
    "def _process_video(frames, roi):\n",
    "    \"\"\"Calculates the average value within the specified ROI for each frame.\"\"\"\n",
    "    RGB = []\n",
    "    for frame in frames:\n",
    "        x, y, w, h = roi\n",
    "        frame_roi = frame[y:y+h, x:x+w]\n",
    "        summation = np.sum(np.sum(frame_roi, axis=0), axis=0)\n",
    "        RGB.append(summation / (frame_roi.shape[0] * frame_roi.shape[1]))\n",
    "    return np.asarray(RGB)\n",
    "\n",
    "def pos_wang(frames, fs=FPS):\n",
    "    frames = np.array(frames)\n",
    "    WinSec = 1.6\n",
    "    N = frames.shape[0]\n",
    "    H = np.zeros((1, N))\n",
    "    l = math.ceil(WinSec * fs)\n",
    "    RGB = None\n",
    "    cumulative_systolic_sum = 0\n",
    "    cumulative_diastolic_sum = 0\n",
    "\n",
    "    for n in range(N):\n",
    "        frame_rgb = frames[n]\n",
    "\n",
    "        if frame_rgb.dtype != np.uint8:\n",
    "            frame_rgb = frame_rgb.astype(np.uint8)\n",
    "\n",
    "        if RGB is None:\n",
    "            face_landmarks_list = face_recognition.face_landmarks(frame_rgb)\n",
    "            if face_landmarks_list:\n",
    "                landmarks = face_landmarks_list[0]\n",
    "                roi = extract_forehead_roi(landmarks)\n",
    "            else:\n",
    "                roi = (0, 0, frame_rgb.shape[1], frame_rgb.shape[0])\n",
    "\n",
    "            RGB = _process_video(frames, roi)\n",
    "\n",
    "        m = n - l\n",
    "        if m >= 0:\n",
    "            Cn = np.true_divide(RGB[m:n, :], np.mean(RGB[m:n, :], axis=0))\n",
    "            Cn = np.asmatrix(Cn).H\n",
    "            S = np.matmul(np.array([[0, 1, -1], [-2, 1, 1]]), Cn)\n",
    "            h = S[0, :] + (np.std(S[0, :]) / np.std(S[1, :])) * S[1, :]\n",
    "            mean_h = np.mean(h)\n",
    "            for temp in range(h.shape[1]):\n",
    "                h[0, temp] = h[0, temp] - mean_h\n",
    "            H[0, m:n] = H[0, m:n] + (h[0])\n",
    "\n",
    "    BVP = H\n",
    "    BVP = detrend(np.asmatrix(BVP).H, 100)\n",
    "    BVP = np.asarray(np.transpose(BVP))[0]\n",
    "    b, a = sp.signal.butter(1, [0.75 / fs * 2, 3 / fs * 2], btype='bandpass')\n",
    "    BVP = sp.signal.filtfilt(b, a, BVP.astype(np.double))\n",
    "\n",
    "    BVP = BVP.copy()\n",
    "\n",
    "    peaks, _ = sp.signal.find_peaks(BVP, distance=fs*0.6)\n",
    "    \n",
    "    RR_intervals = np.diff(peaks) / fs\n",
    "    \n",
    "    avg_peak_to_peak_interval = np.mean(RR_intervals)\n",
    "    \n",
    "    heart_rate = 60 / RR_intervals\n",
    "    \n",
    "    systolic_peaks = BVP[peaks]\n",
    "    diastolic_peaks = BVP[peaks - 1]\n",
    "    \n",
    "    cumulative_systolic_sum = np.sum(systolic_peaks)\n",
    "    cumulative_diastolic_sum = np.sum(diastolic_peaks)\n",
    "    \n",
    "    average_heart_rate = np.mean(heart_rate)\n",
    "\n",
    "    return [average_heart_rate, avg_peak_to_peak_interval, cumulative_systolic_sum, cumulative_diastolic_sum]\n",
    "\n",
    "def extract_forehead_roi(landmarks):\n",
    "    \"\"\"Extracts the ROI for the forehead based on facial landmarks.\"\"\"\n",
    "    forehead_center_x = (landmarks['left_eyebrow'][0][0] + landmarks['right_eyebrow'][-1][0]) // 2\n",
    "    forehead_center_y = landmarks['left_eyebrow'][0][1] - 40\n",
    "    forehead_roi = (forehead_center_x - 50, forehead_center_y - 30, 100, 30)\n",
    "    return forehead_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject_batch(batch, label, n=(FPS*VIDEO_LENGTH)//FRAME_INTERVAL):\n",
    "    ear_choices = np.array([1,2])\n",
    "    eye_gaze_choices = np.array([1,2])\n",
    "    head_pose_choices = np.array([3,4,5,6,7,8])\n",
    "    subject_frames = []\n",
    "    try:\n",
    "        for frame in batch:\n",
    "            frame_features = process_frame(frame)\n",
    "            if frame_features:\n",
    "                subject_frames.append(frame_features)\n",
    "            else:\n",
    "                subject_frames.append([np.random.choice(ear_choices), np.random.choice(eye_gaze_choices), np.random.choice(head_pose_choices)])\n",
    "        max_length = max(len(f) for f in subject_frames) if subject_frames else 0\n",
    "        subject_frames = pad_features(subject_frames, max_length)\n",
    "        subject_frames = np.array(subject_frames)\n",
    "\n",
    "        if isinstance(subject_frames, np.ndarray):\n",
    "            l = subject_frames.shape[0] // n  \n",
    "            subject_frames = subject_frames[:l * n].reshape(l, n, -1)  \n",
    "            subject_frames = mode_value = sp.stats.mode(subject_frames)[0][0]\n",
    "\n",
    "        subject_frames = np.array(subject_frames)\n",
    "        X_batch_N = np.array(pos_wang(batch))\n",
    "        return subject_frames, X_batch_N, label\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        return subject_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch_A = []\n",
    "X_batch_N = []\n",
    "all_labels = []\n",
    "\n",
    "iters = GLOBAL_BATCH_SIZE // BATCH_SIZE\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        X_batch, Y_batch = load_data(PATH, DATASET, BATCH_SIZE)\n",
    "\n",
    "        print(f'{i+1} iterations done')\n",
    "        \n",
    "        for batch, label in zip(X_batch, Y_batch):\n",
    "            futures.append(executor.submit(process_subject_batch, batch, label))\n",
    "\n",
    "    for future in futures:\n",
    "        subject_frames, phys_features, label = future.result()\n",
    "        X_batch_A.append(subject_frames)\n",
    "        X_batch_N.append(phys_features)\n",
    "        all_labels.append(label)\n",
    "\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = np.hstack((X_batch_A, X_batch_N))\n",
    "combined_features_with_labels = np.hstack((combined_features, np.expand_dims(all_labels, axis=-1)))\n",
    "df = pd.DataFrame(combined_features_with_labels, columns=['eye_category', 'eye_position', 'gaze_direction', 'heart_rates', 'p2p_intervals', 'sys_peaks', 'dys_peaks', 'engagement_labels'])\n",
    "\n",
    "filename = f'features_{DATASET}_.csv'\n",
    "df.to_csv(filename, index=False)\n",
    "print(f'Successfully saved to {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
