{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "import face_recognition\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "from imutils import face_utils\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_predictor_path = \"eameo-faceswap-generator/shape_predictor_68_face_landmarks.dat\"\n",
    "try:\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "    print(\"Successfully loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    exit()\n",
    "\n",
    "def eye_aspect_ratio_torch(eye):\n",
    "    A = torch.norm(eye[1] - eye[5])\n",
    "    B = torch.norm(eye[2] - eye[4])\n",
    "    C = torch.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def get_head_pose_torch(shape, size, device='cpu'):\n",
    "    model_points = torch.tensor([\n",
    "        [0.0, 0.0, 0.0], [0.0, -330.0, -65.0], [-225.0, 170.0, -135.0],\n",
    "        [225.0, 170.0, -135.0], [-150.0, -150.0, -125.0], [150.0, -150.0, -125.0]\n",
    "    ], dtype=torch.float32).to(device)\n",
    "\n",
    "    image_points = shape[[30, 8, 36, 45, 48, 54], :]\n",
    "\n",
    "    if image_points.shape[0] < 4:\n",
    "        raise ValueError(\"Not enough points to perform solvePnP\")\n",
    "\n",
    "    focal_length = size[1]\n",
    "    center = (size[1] / 2, size[0] / 2)\n",
    "    camera_matrix = torch.tensor([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=torch.float32).to(device)\n",
    "\n",
    "    dist_coeffs = torch.zeros((4, 1), dtype=torch.float32).to(device)\n",
    "    \n",
    "    image_points_np = image_points.cpu().numpy()\n",
    "    model_points_np = model_points.cpu().numpy()\n",
    "    camera_matrix_np = camera_matrix.cpu().numpy()\n",
    "    dist_coeffs_np = dist_coeffs.cpu().numpy()\n",
    "\n",
    "    success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "        model_points_np, image_points_np, camera_matrix_np, dist_coeffs_np\n",
    "    )\n",
    "\n",
    "    if not success:\n",
    "        raise ValueError(\"solvePnP failed to find a solution\")\n",
    "\n",
    "    rotation_vector = torch.tensor(rotation_vector, dtype=torch.float32).to(device)\n",
    "    translation_vector = torch.tensor(translation_vector, dtype=torch.float32).to(device)\n",
    "    \n",
    "    return rotation_vector, translation_vector\n",
    "\n",
    "def rotation_vector_to_euler_angles(rotation_vector):\n",
    "    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "    sy = np.sqrt(rotation_matrix[2, 1] ** 2 + rotation_matrix[2, 2] ** 2)\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if not singular:\n",
    "        x = np.arctan2(rotation_matrix[2, 1], rotation_matrix[2, 2])\n",
    "        y = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "        z = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
    "    else:\n",
    "        x = np.arctan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])\n",
    "        y = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.degrees(x), np.degrees(y), np.degrees(z)\n",
    "\n",
    "def pad_features(features, target_length):\n",
    "    \"\"\"\n",
    "    Pads or truncates each feature vector in `features` to `target_length`.\n",
    "\n",
    "    Parameters:\n",
    "    - features: List of feature vectors, each of which can be a list or tuple.\n",
    "    - target_length: Desired length of each feature vector.\n",
    "\n",
    "    Returns:\n",
    "    - List of padded or truncated feature vectors.\n",
    "    \"\"\"\n",
    "    padded_features = []\n",
    "    for feature_vector in features:\n",
    "        if not isinstance(feature_vector, (list, tuple)):\n",
    "            raise ValueError(\"Each feature vector should be a list or tuple.\")\n",
    "        \n",
    "        if isinstance(feature_vector, tuple):\n",
    "            feature_vector = list(feature_vector)\n",
    "        \n",
    "        if len(feature_vector) < target_length:\n",
    "            padded_vector = feature_vector + [0] * (target_length - len(feature_vector))\n",
    "        else:\n",
    "            padded_vector = feature_vector[:target_length]\n",
    "        \n",
    "        padded_features.append(padded_vector)\n",
    "    \n",
    "    return padded_features\n",
    "\n",
    "\n",
    "def classify_eye_openness(ear, threshold_open=0.35, threshold_closed=0.2):\n",
    "    if ear >= 0.5:\n",
    "        return 0\n",
    "    if ear > threshold_open:\n",
    "        return 0 \n",
    "    elif threshold_closed <= ear <= threshold_open:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2 \n",
    "\n",
    "def compute_gaze_direction(shape_tensor, rotation_vector, image_size):\n",
    "    left_eye_landmarks = shape_tensor[36:42]\n",
    "    right_eye_landmarks = shape_tensor[42:48]\n",
    "    \n",
    "    left_eye_center = left_eye_landmarks.mean(dim=0)\n",
    "    right_eye_center = right_eye_landmarks.mean(dim=0)\n",
    "    \n",
    "    eye_center = (left_eye_center + right_eye_center) / 2\n",
    "    \n",
    "    image_center = torch.tensor([image_size[1] / 2, image_size[0] / 2], dtype=torch.float32)\n",
    "    \n",
    "    gaze_vector = image_center - eye_center[:2]\n",
    "    \n",
    "    pitch, yaw, roll = rotation_vector_to_euler_angles(rotation_vector.cpu().numpy())\n",
    "    \n",
    "    return pitch, yaw, gaze_vector.tolist()\n",
    "\n",
    "def classify_head_pose(pitch, yaw, roll, gaze_vector):\n",
    "    \"\"\"\n",
    "    Classifies head pose based on pitch, yaw, roll, and gaze direction.\n",
    "    \n",
    "    Parameters:\n",
    "        pitch (float): The pitch angle.\n",
    "        yaw (float): The yaw angle.\n",
    "        roll (float): The roll angle.\n",
    "        gaze_vector (list): The gaze direction vector [x, y].\n",
    "    \n",
    "    Returns:\n",
    "        str: A classification string representing the head pose.\n",
    "    \"\"\"\n",
    "    gaze_direction = 0\n",
    "    if gaze_vector[0] > 0.5:\n",
    "        gaze_direction = 1 \n",
    "    elif gaze_vector[0] < -0.5:\n",
    "        gaze_direction = 2 \n",
    "    \n",
    "    if (pitch > 10 and pitch <= 180) or (pitch <= -180):\n",
    "        position = 3 \n",
    "    elif (pitch < -10 and pitch > -180) or (pitch > 180):\n",
    "        position = 4 \n",
    "    elif yaw > 10:\n",
    "        position = 5\n",
    "    elif yaw < -10:\n",
    "        position = 6 \n",
    "    elif roll > 10:\n",
    "        position = 7\n",
    "    elif roll < -10:\n",
    "        position = 8\n",
    "    else:\n",
    "        position = 0\n",
    "    \n",
    "    return position, gaze_direction\n",
    "\n",
    "def process_frame(frame):\n",
    "    frame_np = frame\n",
    "    gray = cv2.cvtColor(frame_np, cv2.COLOR_RGB2GRAY)\n",
    "    rects = detector(gray, 0)\n",
    "    features = []\n",
    "\n",
    "    if len(rects) == 0:\n",
    "        return features\n",
    "\n",
    "    shapes = [face_utils.shape_to_np(predictor(gray, rect)) for rect in rects]\n",
    "    max_width = 0\n",
    "    best_features = None\n",
    "    MIN_FACE_SIZE = 5000\n",
    "    for i, rect in enumerate(rects):\n",
    "        if rect.width() * rect.height() < MIN_FACE_SIZE:\n",
    "            continue \n",
    "        shape = shapes[i]\n",
    "        shape_tensor = torch.tensor(shape, dtype=torch.float32)\n",
    "\n",
    "        leftEye = shape_tensor[42:48]\n",
    "        rightEye = shape_tensor[36:42]\n",
    "        ear = (eye_aspect_ratio_torch(leftEye) + eye_aspect_ratio_torch(rightEye)) / 2.0\n",
    "\n",
    "        size = (frame_np.shape[1], frame_np.shape[0])\n",
    "        rotation_vector, translation_vector = get_head_pose_torch(shape_tensor, size)\n",
    "        pitch, yaw, roll = rotation_vector_to_euler_angles(rotation_vector.numpy())\n",
    "        gaze_vector = compute_gaze_direction(shape_tensor, rotation_vector, size)\n",
    "        position, gaze_direction = classify_head_pose(pitch, yaw, roll, gaze_vector)\n",
    "        eye_category = classify_eye_openness(ear.item())\n",
    "\n",
    "        pitch = np.clip(pitch, -90, 90)\n",
    "\n",
    "        current_features = [eye_category, position, gaze_direction, ear.item(), pitch, yaw, roll]\n",
    "\n",
    "        face_width = rect.width() * rect.height()\n",
    "        if face_width > max_width:\n",
    "            max_width = face_width\n",
    "            best_features = current_features\n",
    "\n",
    "    if best_features is not None:\n",
    "        features = best_features\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forehead_roi_from_frame(frame):\n",
    "    frame = frame.numpy()\n",
    "    if not isinstance(frame, np.ndarray):\n",
    "        raise ValueError(\"Frame must be a NumPy array.\")\n",
    "\n",
    "    frame_rgb = frame.astype(np.uint8)\n",
    "    face_locations = face_recognition.face_locations(frame_rgb)\n",
    "    roi = (0, 0, frame.shape[0], frame.shape[1])\n",
    "\n",
    "    if face_locations:\n",
    "        top, right, bottom, left = face_locations[0]\n",
    "        forehead_height = (bottom - top) // 5\n",
    "        roi = (left, top, right, top + forehead_height)\n",
    "\n",
    "    return roi\n",
    "\n",
    "def detrend(input_signal, lambda_value):\n",
    "    signal_length = input_signal.shape[0]\n",
    "    H = np.identity(signal_length)\n",
    "    ones = np.ones(signal_length)\n",
    "    minus_twos = -2 * np.ones(signal_length)\n",
    "    diags_data = np.array([ones, minus_twos, ones])\n",
    "    diags_index = np.array([0, 1, 2])\n",
    "    D = sp.sparse.spdiags(diags_data, diags_index, (signal_length - 2), signal_length).toarray()\n",
    "    filtered_signal = np.dot(\n",
    "        (H - np.linalg.inv(H + (lambda_value ** 2) * np.dot(D.T, D))), input_signal)\n",
    "    return filtered_signal\n",
    "\n",
    "def _process_video(frames):\n",
    "    if isinstance(frames, torch.Tensor):\n",
    "        frames = frames.numpy() \n",
    "\n",
    "    RGB = []\n",
    "    for frame in frames:\n",
    "        frame_tensor = torch.tensor(frame, dtype=torch.float32)\n",
    "        summation = torch.sum(frame_tensor, dim=(0, 1)).numpy()\n",
    "        RGB.append(summation / (frame_tensor.shape[0] * frame_tensor.shape[1]))\n",
    "    \n",
    "    return np.asarray(RGB)\n",
    "\n",
    "def green_pos_features(frames, fps=30):\n",
    "    if isinstance(frames, list):\n",
    "        frames = torch.tensor(np.array(frames), dtype=torch.float32)\n",
    "    elif isinstance(frames, np.ndarray):\n",
    "        frames = torch.tensor(frames, dtype=torch.float32)\n",
    "    frames = frames.float()\n",
    "    \n",
    "    bvp_no_roi = []\n",
    "    for frame in frames:\n",
    "        roi_frame = frame[:, :, 1]\n",
    "        bvp_no_roi.append(torch.mean(roi_frame).item())\n",
    "    bvp_no_roi = np.array(bvp_no_roi)\n",
    "\n",
    "    bvp_roi = []\n",
    "    for frame in frames:\n",
    "        roi = get_forehead_roi_from_frame(frame)\n",
    "        left, top, right, bottom = roi\n",
    "        roi_frame = frame[top:bottom, left:right, 1]\n",
    "        bvp_roi.append(torch.mean(roi_frame).item())\n",
    "    bvp_roi = np.array(bvp_roi)\n",
    "\n",
    "    LPF = 0.7\n",
    "    HPF = 2.5\n",
    "    RGB = _process_video(frames)\n",
    "    N = RGB.shape[0]\n",
    "    H = np.zeros((1, N))\n",
    "    WinSec = 1.6\n",
    "    l = math.ceil(WinSec * fps)\n",
    "\n",
    "    for n in range(N):\n",
    "        m = n - l\n",
    "        if m >= 0:\n",
    "            Cn = np.true_divide(RGB[m:n, :], np.mean(RGB[m:n, :], axis=0))\n",
    "            Cn = np.asmatrix(Cn).H\n",
    "            S = np.matmul(np.array([[0, 1, -1], [-2, 1, 1]]), Cn)\n",
    "            h = S[0, :] + (np.std(S[0, :]) / np.std(S[1, :])) * S[1, :]\n",
    "            mean_h = np.mean(h)\n",
    "            for temp in range(h.shape[1]):\n",
    "                h[0, temp] = h[0, temp] - mean_h\n",
    "            H[0, m:n] = H[0, m:n] + (h[0])\n",
    "\n",
    "    BVP_pos = H\n",
    "    BVP_pos = detrend(np.asmatrix(BVP_pos).H, 100)\n",
    "    BVP_pos = np.asarray(np.transpose(BVP_pos))[0]\n",
    "    b, a = sp.signal.butter(1, [0.75 / fps * 2, 3 / fps * 2], btype='bandpass')\n",
    "    BVP_pos = sp.signal.filtfilt(b, a, BVP_pos.astype(np.double))\n",
    "    BVP_pos = BVP_pos.copy()\n",
    "\n",
    "    nyquistF = fps / 2\n",
    "    lowCutOffFreq = LPF / nyquistF\n",
    "    highCutOffFreq = HPF / nyquistF\n",
    "    b, a = sp.signal.butter(3, [lowCutOffFreq, highCutOffFreq], btype='band')\n",
    "    bvp_no_roi_filtered = sp.signal.filtfilt(b, a, bvp_no_roi - np.mean(bvp_no_roi, axis=-1, keepdims=True), axis=-1)\n",
    "    bvp_roi_filtered = sp.signal.filtfilt(b, a, bvp_roi - np.mean(bvp_roi, axis=-1, keepdims=True), axis=-1)\n",
    "\n",
    "    def calculate_features(bvp_filtered, fps = 30):\n",
    "        peaks, _ = sp.signal.find_peaks(bvp_filtered, distance=fps*0.6) \n",
    "        RR_intervals = np.diff(peaks) / fps\n",
    "        heart_rate = 60 / RR_intervals\n",
    "        average_heart_rate = np.mean(heart_rate)\n",
    "        systolic_peaks, _ = sp.signal.find_peaks(bvp_filtered)\n",
    "        diastolic_peaks, _ = sp.signal.find_peaks(-bvp_filtered)\n",
    "        if len(systolic_peaks) > 1:\n",
    "            peak_intervals = np.diff(systolic_peaks) / fps\n",
    "            avg_peak_interval = np.mean(peak_intervals)\n",
    "        else:\n",
    "            avg_peak_interval = np.nan\n",
    "        systolic_peak_sum = np.sum(bvp_filtered[systolic_peaks])\n",
    "        diastolic_peak_sum = np.sum(-bvp_filtered[diastolic_peaks])\n",
    "        \n",
    "\n",
    "        return avg_peak_interval, systolic_peak_sum, diastolic_peak_sum,average_heart_rate\n",
    "\n",
    "    features_no_roi = np.array(calculate_features(bvp_no_roi_filtered))\n",
    "    features_roi = np.array(calculate_features(bvp_roi_filtered))\n",
    "    features_pos = np.array(calculate_features(BVP_pos))\n",
    "\n",
    "    return np.concatenate([features_no_roi, features_roi, features_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject_batch(batch, label, n=(FPS*VIDEO_LENGTH)//(FRAME_INTERVAL)):\n",
    "    ear_choices = np.array([1, 2])\n",
    "    eye_gaze_choices = np.array([1, 2])\n",
    "    head_pose_choices = np.array([3, 4, 5, 6, 7, 8])\n",
    "    subject_frames = []\n",
    "    \n",
    "    try:\n",
    "        for frame in batch:\n",
    "            frame_features = process_frame(frame)\n",
    "            if frame_features:\n",
    "                subject_frames.append(frame_features)\n",
    "            else:\n",
    "                subject_frames.append([np.random.choice(ear_choices), np.random.choice(eye_gaze_choices), np.random.choice(head_pose_choices),0, 0, 0, 0 ])\n",
    "        \n",
    "        if subject_frames:\n",
    "            max_length = max(len(f) for f in subject_frames) if subject_frames else 0\n",
    "            subject_frames = pad_features(subject_frames, max_length)\n",
    "            subject_frames = np.array(subject_frames)\n",
    "        else:\n",
    "            subject_frames = np.zeros((n, 7))  \n",
    "            \n",
    "        \n",
    "        if isinstance(subject_frames, np.ndarray) and subject_frames.size > 0:\n",
    "            l = subject_frames.shape[0] // n\n",
    "            subject_frames = subject_frames[:l * n].reshape(l, n, -1)\n",
    "\n",
    "            categorical_features = subject_frames[:, :, :3]\n",
    "            numerical_features = subject_frames[:, :, 3:]\n",
    "\n",
    "            mode_categorical = sp.stats.mode(categorical_features, axis=1, keepdims=False).mode\n",
    "\n",
    "            mean_numerical = numerical_features.mean(axis=1)\n",
    "\n",
    "            subject_frames = np.concatenate((mode_categorical, mean_numerical), axis=1)\n",
    "\n",
    "        X_batch_N = np.array(green_pos_features(batch))\n",
    "\n",
    "        return subject_frames, X_batch_N, label\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        return np.array([]), np.array([]), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch_A = []\n",
    "X_batch_N = []\n",
    "all_labels = []\n",
    "\n",
    "iters = GLOBAL_BATCH_SIZE // BATCH_SIZE\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=256) as executor:\n",
    "    futures = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        X_batch, Y_batch = load_data(PATH, DATASET, BATCH_SIZE)\n",
    "\n",
    "        print(f'{i+1} iterations done')\n",
    "        \n",
    "        for batch, label in zip(X_batch, Y_batch):\n",
    "            futures.append(executor.submit(process_subject_batch, batch, label))\n",
    "\n",
    "    for future in futures:\n",
    "        subject_frames, phys_features, label = future.result()\n",
    "        X_batch_A.append(subject_frames)\n",
    "        X_batch_N.append(phys_features)\n",
    "        all_labels.append(label)\n",
    "\n",
    "X_batch_A = np.array(X_batch_A)\n",
    "X_batch_A = np.squeeze(X_batch_A, axis=1)\n",
    "X_batch_N = np.array(X_batch_N)\n",
    "\n",
    "X_batch_A_df = pd.DataFrame(X_batch_A)\n",
    "X_batch_N_df = pd.DataFrame(X_batch_N)\n",
    "\n",
    "\n",
    "X_batch_A_df.fillna(X_batch_A_df.median(numeric_only=True), inplace=True)\n",
    "X_batch_N_df.fillna(X_batch_N_df.median(numeric_only=True), inplace=True)\n",
    "\n",
    "X_batch_A = X_batch_A_df.values\n",
    "X_batch_N = X_batch_N_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = np.hstack((X_batch_A, X_batch_N))\n",
    "combined_features_with_labels = np.hstack((combined_features,all_labels))\n",
    "df = pd.DataFrame(combined_features_with_labels, columns=['eye_category', 'eye_position', 'gaze_direction', 'ear', 'pitch', 'yaw', 'roll', 'features_no_roi_avg_peak_interval', 'features_no_roi_systolic_peak_sum', 'features_no_roi_diastolic_peak_sum', 'features_no_roi_average_heart_rate', 'features_roi_avg_peak_interval', 'features_roi_systolic_peak_sum', 'features_roi_diastolic_peak_sum', 'features_roi_average_heart_rate', 'features_pos_avg_peak_interval', 'features_pos_systolic_peak_sum', 'features_pos_diastolic_peak_sum', 'features_pos_average_heart_rate', 'engagement_labels'])\n",
    "\n",
    "filename = f'features_{DATASET}_ablation_.csv'\n",
    "df.to_csv(filename, index=False)\n",
    "print(f'Successfully saved to {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
